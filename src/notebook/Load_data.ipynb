{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4022bce",
   "metadata": {},
   "source": [
    "# Load\n",
    "\n",
    "This notebook is used to demonstrate ingestion of data, refer to cell #2 for configuration values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f142c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Initialization"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CURRENT_USER()</th>\n",
       "      <th>CURRENT_ROLE()</th>\n",
       "      <th>CURRENT_DATABASE()</th>\n",
       "      <th>CURRENT_SCHEMA()</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SOLNDEMOUSR</td>\n",
       "      <td>PUBLIC</td>\n",
       "      <td>INDSOL_CMSGOV_PRICING</td>\n",
       "      <td>PUBLIC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CURRENT_USER() CURRENT_ROLE()     CURRENT_DATABASE() CURRENT_SCHEMA()\n",
       "0    SOLNDEMOUSR         PUBLIC  INDSOL_CMSGOV_PRICING           PUBLIC"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialization block\n",
    "from IPython.display import display, HTML, Image , Markdown\n",
    "from snowflake.snowpark.session import Session\n",
    "import snowflake.snowpark.types as T\n",
    "import snowflake.snowpark.functions as F\n",
    "import os ,configparser ,json ,logging\n",
    "\n",
    "# Import the commonly defined utility scripts using\n",
    "# dynamic path include\n",
    "import sys\n",
    "sys.path.append('../python/lutils')\n",
    "import sflk_base as L\n",
    "\n",
    "display(Markdown(\"### Initialization\"))\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.ERROR)\n",
    "\n",
    "# Source various helper functions\n",
    "%run ./scripts/notebook_helpers.py\n",
    "\n",
    "# Define the project home directory, this is used for locating the config.ini file\n",
    "PROJECT_HOME_DIR = '../../'\n",
    "config = L.get_config(PROJECT_HOME_DIR)\n",
    "sp_session = L.connect_to_snowflake(PROJECT_HOME_DIR)\n",
    "\n",
    "if(sp_session == None):\n",
    "    raise Exception(f'Unable to connect to snowflake. Validate connection information ')\n",
    "\n",
    "sp_session.use_role(f'''{config['APP_DB']['role']}''')\n",
    "sp_session.use_schema(f'''{config['APP_DB']['database']}.{config['APP_DB']['schema']}''')\n",
    "sp_session.use_warehouse(f'''{config['SNOW_CONN']['warehouse']}''')\n",
    "\n",
    "df = sp_session.sql('select current_user() ,current_role() ,current_database() ,current_schema();').to_pandas()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90cd9156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter initialization\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def get_file_from_download_url(p_fl_url):\n",
    "    splits = p_fl_url.split('/')\n",
    "    return splits[len(splits) - 1]\n",
    "\n",
    "def get_basename_of_datafile(p_datafile:str) -> str:\n",
    "    base = os.path.basename(p_datafile)\n",
    "    fl_base = os.path.splitext(base)\n",
    "    return fl_base[0]\n",
    "\n",
    "def get_cleansed_file_basename(p_datafile):\n",
    "    fl_basename = get_basename_of_datafile(p_datafile)\n",
    "    # Replace all non alphanumeric characters with _\n",
    "    fl_name = re.sub('[^0-9a-zA-Z]+', '_', fl_basename)\n",
    "    return fl_name\n",
    "\n",
    "DATA_FILE_URL = 'https://priorityhealthtransparencymrfs.s3.amazonaws.com/2023_03_01_priority_health_HMO_in-network-rates.zip'\n",
    "DATA_FILE = get_file_from_download_url(DATA_FILE_URL)\n",
    "DATA_FILE_BASENAME = get_basename_of_datafile(DATA_FILE)\n",
    "DATA_FILE_BASENAME_CLEANSED = get_cleansed_file_basename(DATA_FILE)\n",
    "\n",
    "# INPUT_DATA_STAGE = config['APP_DB']['ext_stage']\n",
    "INPUT_DATA_STAGE = 'data_stg'\n",
    "DATA_STAGE_FOLDER = config['APP_DB']['folder_data']\n",
    "\n",
    "TARGET_DATA_STAGE = config['APP_DB']['ext_stage']\n",
    "TARGET_FOLDER = config['APP_DB']['folder_parsed']\n",
    "\n",
    "# This will need to be updated based on provider\n",
    "# Priority Health ~ 500\n",
    "# CIGNA ~ 15000\n",
    "SEGMENTS_PER_TASK = 500\n",
    "\n",
    "warehouses = config['SNOW_CONN']['warehouse']\n",
    "create_warehouses = True\n",
    "warehouses_count = 10\n",
    "#warehouses = 'INDSOL_PRICE_TRANS_TASK_0_WH,INDSOL_PRICE_TRANS_TASK_1_WH,INDSOL_PRICE_TRANS_TASK_2_WH,INDSOL_PRICE_TRANS_TASK_3_WH,INDSOL_PRICE_TRANS_TASK_4_WH,INDSOL_PRICE_TRANS_TASK_5_WH,INDSOL_PRICE_TRANS_TASK_6_WH,INDSOL_PRICE_TRANS_TASK_7_WH,INDSOL_PRICE_TRANS_TASK_8_WH,INDSOL_PRICE_TRANS_TASK_9_WH,INDSOL_PRICE_TRANS_TASK_10_WH'\n",
    "#warehouses = 'INDSOL_PRICE_TRANS_TASK_0_WH,INDSOL_PRICE_TRANS_TASK_1_WH,INDSOL_PRICE_TRANS_TASK_2_WH,INDSOL_PRICE_TRANS_TASK_3_WH,INDSOL_PRICE_TRANS_TASK_4_WH,INDSOL_PRICE_TRANS_TASK_5_WH'\n",
    "warehouses_lst = [ f'INDSOL_PRICE_TRANS_TASK_{i}_WH' for i in range(warehouses_count) ]\n",
    "warehouses = ','.join(warehouses_lst)\n",
    "\n",
    "# XSMALL | SMALL | MEDIUM | LARGE | XLARGE | XXLARGE | XXXLARGE | X4LARGE | X5LARGE | X6LARGE\n",
    "warehouse_size = 'MEDIUM'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b5e80d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download file and upload to stage\n",
    "\n",
    "from urllib import request\n",
    "\n",
    "# Create local download folder\n",
    "DOWNLOAD_FOLDER=f'{PROJECT_HOME_DIR}/temp'\n",
    "os.makedirs(DOWNLOAD_FOLDER ,exist_ok=True)\n",
    "download_file_path = os.path.join(DOWNLOAD_FOLDER, DATA_FILE)\n",
    "\n",
    "if not os.path.exists(os.path.dirname(download_file_path)):\n",
    "    print(f'Downloading file to local: {download_file_path}')\n",
    "    request.urlretrieve(DATA_FILE_URL, download_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e0bd458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RELATIVE_PATH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/2023_03_01_priority_health_HMO_in-network-rates.zip</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              RELATIVE_PATH\n",
       "0  data/2023_03_01_priority_health_HMO_in-network-rates.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Upload data file to stage, if not present\n",
    "\n",
    "sql_stmt = f'''select relative_path\n",
    "from directory(@{INPUT_DATA_STAGE})\n",
    "where relative_path like '%{DATA_FILE}%'\n",
    ";'''\n",
    "# print(sql_stmt)\n",
    "\n",
    "sp_session.sql(f'alter stage {INPUT_DATA_STAGE} refresh;').collect()\n",
    "df = sp_session.sql(sql_stmt).to_pandas()\n",
    "if (len(df) < 1):\n",
    "    print(f'Uploading to stage {INPUT_DATA_STAGE} ...')\n",
    "    sp_session.file.put(\n",
    "        local_file_name = download_file_path\n",
    "        ,stage_location = f'{INPUT_DATA_STAGE}/{DATA_STAGE_FOLDER}'\n",
    "        ,auto_compress=False ,overwrite=True)\n",
    "    \n",
    "    sp_session.sql('alter stage {INPUT_DATA_STAGE} refresh;').collect()\n",
    "    df = sp_session.sql(sql_stmt).to_pandas()\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4061228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 10 warehouses ..\n",
      "    - INDSOL_PRICE_TRANS_TASK_0_WH\n",
      "    - INDSOL_PRICE_TRANS_TASK_1_WH\n",
      "    - INDSOL_PRICE_TRANS_TASK_2_WH\n",
      "    - INDSOL_PRICE_TRANS_TASK_3_WH\n",
      "    - INDSOL_PRICE_TRANS_TASK_4_WH\n",
      "    - INDSOL_PRICE_TRANS_TASK_5_WH\n",
      "    - INDSOL_PRICE_TRANS_TASK_6_WH\n",
      "    - INDSOL_PRICE_TRANS_TASK_7_WH\n",
      "    - INDSOL_PRICE_TRANS_TASK_8_WH\n",
      "    - INDSOL_PRICE_TRANS_TASK_9_WH\n"
     ]
    }
   ],
   "source": [
    "# Create warehouses for parallelism\n",
    "\n",
    "if create_warehouses == True:\n",
    "    whs = warehouses.split(',')\n",
    "    print(f'Creating {len(whs)} warehouses ..')\n",
    "\n",
    "    sp_session.sql('use role sysadmin;').collect()\n",
    "    for wh_name in whs:\n",
    "        print(f'    - {wh_name}')\n",
    "        sql_stmt = f'''\n",
    "            create warehouse if not exits {wh_name} with\n",
    "                WAREHOUSE_SIZE = 'XSMALL'\n",
    "                AUTO_RESUME = TRUE\n",
    "                AUTO_SUSPEND = 300\n",
    "                COMMENT = 'warehouse created as part of pricing transperancy industry solution usecase.'\n",
    "            ;\n",
    "        '''\n",
    "        sp_session.sql(sql_stmt).collect()\n",
    "        rl = config['APP_DB']['role']\n",
    "        sp_session.sql(f'grant ALL PRIVILEGES on warehouse {wh_name} to role {rl};').collect()\n",
    "       \n",
    "    sp_session.use_role(f'''{config['APP_DB']['role']}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f908941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " truncating tables ...\n",
      " cleaning up files in external stage under path raw_parsed/2023_03_01_priority_health_HMO_in-network-rates/ ...\n"
     ]
    }
   ],
   "source": [
    "# Cleanup block\n",
    "\n",
    "# We will cleanup specific resources and artifacts from possible previous runs.\n",
    "stmts = [\n",
    "    f''' delete from segment_task_execution_status where data_file = '{DATA_FILE}'; '''\n",
    "    ,f''' delete from task_to_segmentids where data_file = '{DATA_FILE}'; '''\n",
    "    ,f''' delete from in_network_rates_file_header where data_file = '{DATA_FILE}'; '''\n",
    "    ,f''' delete from in_network_rates_segment_header where data_file = '{DATA_FILE}'; '''\n",
    "    ,f''' alter stage {INPUT_DATA_STAGE} refresh; '''\n",
    "]    \n",
    "    \n",
    "print(' truncating tables ...')\n",
    "for stmt in stmts:\n",
    "    sp_session.sql(stmt).collect()\n",
    "\n",
    "print(f''' cleaning up files in external stage under path {TARGET_FOLDER}/{DATA_FILE_BASENAME}/ ...''')\n",
    "\n",
    "stmt = f''' select relative_path from directory(@{TARGET_DATA_STAGE}) where relative_path like '%{DATA_STAGE_FOLDER}/{DATA_FILE_BASENAME}/%'; '''\n",
    "files = sp_session.sql(stmt).collect()\n",
    "for r in files:\n",
    "    stmt = f''' remove @{TARGET_DATA_STAGE}/{r['RELATIVE_PATH']}; '''\n",
    "    sp_session.sql(stmt).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2e44efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning dags for datafile: 2023_03_01_priority_health_HMO_in_network_rates\n",
      "ERROR:snowflake.snowpark._internal.server_connection:Failed to execute query [queryID: 01ab15d2-0402-b697-0057-3c03055c8bee] CALL delete_dag_for_datafile('2023_03_01_priority_health_HMO_in_network_rates', False :: BOOLEAN)\n",
      "100357 (P0000): Python Interpreter Error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/udf/374669008373/delete_dag_for_datafile.py\", line 54, in main\n",
      "    tasks_affected = delete_taskdefinitions_for_datafile(p_session ,p_datafile ,p_drop_tasks)\n",
      "  File \"/home/udf/374669008373/delete_dag_for_datafile.py\", line 46, in delete_taskdefinitions_for_datafile\n",
      "    tasks_dropped.append(r['TASK_NAME'])\n",
      "UnboundLocalError: local variable 'r' referenced before assignment\n",
      " in function DELETE_DAG_FOR_DATAFILE with handler delete_dag_for_datafile.main\n"
     ]
    },
    {
     "ename": "SnowparkSQLException",
     "evalue": "(1304): 01ab15d2-0402-b697-0057-3c03055c8bee: 100357 (P0000): Python Interpreter Error:\nTraceback (most recent call last):\n  File \"/home/udf/374669008373/delete_dag_for_datafile.py\", line 54, in main\n    tasks_affected = delete_taskdefinitions_for_datafile(p_session ,p_datafile ,p_drop_tasks)\n  File \"/home/udf/374669008373/delete_dag_for_datafile.py\", line 46, in delete_taskdefinitions_for_datafile\n    tasks_dropped.append(r['TASK_NAME'])\nUnboundLocalError: local variable 'r' referenced before assignment\n in function DELETE_DAG_FOR_DATAFILE with handler delete_dag_for_datafile.main",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSnowparkSQLException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Cautious enablement, used during development for testing\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCleaning dags for datafile: \u001b[39m\u001b[39m{\u001b[39;00mDATA_FILE_BASENAME_CLEANSED\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m sp_session\u001b[39m.\u001b[39;49mcall(\u001b[39m'\u001b[39;49m\u001b[39mdelete_dag_for_datafile\u001b[39;49m\u001b[39m'\u001b[39;49m,DATA_FILE_BASENAME_CLEANSED ,\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pysnowpark/lib/python3.8/site-packages/snowflake/snowpark/session.py:1801\u001b[0m, in \u001b[0;36mSession.call\u001b[0;34m(self, sproc_name, *args)\u001b[0m\n\u001b[1;32m   1799\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msql(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCALL \u001b[39m\u001b[39m{\u001b[39;00msproc_name\u001b[39m}\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(sql_args)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1800\u001b[0m set_api_call_source(df, \u001b[39m\"\u001b[39m\u001b[39mSession.call\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1801\u001b[0m \u001b[39mreturn\u001b[39;00m df\u001b[39m.\u001b[39;49mcollect()[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pysnowpark/lib/python3.8/site-packages/snowflake/snowpark/_internal/telemetry.py:138\u001b[0m, in \u001b[0;36mdf_collect_api_telemetry.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    137\u001b[0m     \u001b[39mwith\u001b[39;00m args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_session\u001b[39m.\u001b[39mquery_history() \u001b[39mas\u001b[39;00m query_history:\n\u001b[0;32m--> 138\u001b[0m         result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    139\u001b[0m     plan \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_select_statement \u001b[39mor\u001b[39;00m args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_plan\n\u001b[1;32m    140\u001b[0m     api_calls \u001b[39m=\u001b[39m [\n\u001b[1;32m    141\u001b[0m         \u001b[39m*\u001b[39mplan\u001b[39m.\u001b[39mapi_calls,\n\u001b[1;32m    142\u001b[0m         {TelemetryField\u001b[39m.\u001b[39mNAME\u001b[39m.\u001b[39mvalue: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataFrame.\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    143\u001b[0m     ]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pysnowpark/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py:549\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self, statement_params, block)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[39m@df_collect_api_telemetry\u001b[39m\n\u001b[1;32m    534\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcollect\u001b[39m(\n\u001b[1;32m    535\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m, statement_params: Optional[Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, block: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    536\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[List[Row], AsyncJob]:\n\u001b[1;32m    537\u001b[0m     \u001b[39m\"\"\"Executes the query representing this DataFrame and returns the result as a\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[39m    list of :class:`Row` objects.\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[39m        :meth:`collect_nowait()`\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 549\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_internal_collect_with_tag_no_telemetry(\n\u001b[1;32m    550\u001b[0m         statement_params\u001b[39m=\u001b[39;49mstatement_params, block\u001b[39m=\u001b[39;49mblock\n\u001b[1;32m    551\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pysnowpark/lib/python3.8/site-packages/snowflake/snowpark/dataframe.py:584\u001b[0m, in \u001b[0;36mDataFrame._internal_collect_with_tag_no_telemetry\u001b[0;34m(self, statement_params, block, data_type)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_internal_collect_with_tag_no_telemetry\u001b[39m(\n\u001b[1;32m    575\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    576\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[39m# we should always call this method instead of collect(), to make sure the\u001b[39;00m\n\u001b[1;32m    583\u001b[0m     \u001b[39m# query tag is set properly.\u001b[39;00m\n\u001b[0;32m--> 584\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session\u001b[39m.\u001b[39;49m_conn\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    585\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plan,\n\u001b[1;32m    586\u001b[0m         block\u001b[39m=\u001b[39;49mblock,\n\u001b[1;32m    587\u001b[0m         data_type\u001b[39m=\u001b[39;49mdata_type,\n\u001b[1;32m    588\u001b[0m         _statement_params\u001b[39m=\u001b[39;49mcreate_or_update_statement_params_with_query_tag(\n\u001b[1;32m    589\u001b[0m             statement_params, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session\u001b[39m.\u001b[39;49mquery_tag, SKIP_LEVELS_THREE\n\u001b[1;32m    590\u001b[0m         ),\n\u001b[1;32m    591\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pysnowpark/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py:406\u001b[0m, in \u001b[0;36mServerConnection.execute\u001b[0;34m(self, plan, to_pandas, to_iter, block, data_type, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39mif\u001b[39;00m is_in_stored_procedure() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m block:  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    403\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    404\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsync query is not supported in stored procedure yet\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    405\u001b[0m     )\n\u001b[0;32m--> 406\u001b[0m result_set, result_meta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_result_set(\n\u001b[1;32m    407\u001b[0m     plan, to_pandas, to_iter, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, block\u001b[39m=\u001b[39;49mblock, data_type\u001b[39m=\u001b[39;49mdata_type\n\u001b[1;32m    408\u001b[0m )\n\u001b[1;32m    409\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m block:\n\u001b[1;32m    410\u001b[0m     \u001b[39mreturn\u001b[39;00m result_set\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pysnowpark/lib/python3.8/site-packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py:154\u001b[0m, in \u001b[0;36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     ne \u001b[39m=\u001b[39m SnowparkClientExceptionMessages\u001b[39m.\u001b[39mSQL_EXCEPTION_FROM_PROGRAMMING_ERROR(\n\u001b[1;32m    152\u001b[0m         e\n\u001b[1;32m    153\u001b[0m     )\n\u001b[0;32m--> 154\u001b[0m     \u001b[39mraise\u001b[39;00m ne\u001b[39m.\u001b[39mwith_traceback(tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pysnowpark/lib/python3.8/site-packages/snowflake/snowpark/_internal/analyzer/snowflake_plan.py:87\u001b[0m, in \u001b[0;36mSnowflakePlan.Decorator.wrap_exception.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     86\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     88\u001b[0m     \u001b[39mexcept\u001b[39;00m snowflake\u001b[39m.\u001b[39mconnector\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mProgrammingError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     89\u001b[0m         tb \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mexc_info()[\u001b[39m2\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pysnowpark/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py:494\u001b[0m, in \u001b[0;36mServerConnection.get_result_set\u001b[0;34m(self, plan, to_pandas, to_iter, block, data_type, **kwargs)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[39mfor\u001b[39;00m holder, id_ \u001b[39min\u001b[39;00m placeholders\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    493\u001b[0m     final_query \u001b[39m=\u001b[39m final_query\u001b[39m.\u001b[39mreplace(holder, id_)\n\u001b[0;32m--> 494\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_query(\n\u001b[1;32m    495\u001b[0m     final_query,\n\u001b[1;32m    496\u001b[0m     to_pandas,\n\u001b[1;32m    497\u001b[0m     to_iter \u001b[39mand\u001b[39;49;00m (i \u001b[39m==\u001b[39;49m \u001b[39mlen\u001b[39;49m(plan\u001b[39m.\u001b[39;49mqueries) \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m),\n\u001b[1;32m    498\u001b[0m     is_ddl_on_temp_object\u001b[39m=\u001b[39;49mquery\u001b[39m.\u001b[39;49mis_ddl_on_temp_object,\n\u001b[1;32m    499\u001b[0m     block\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m is_last,\n\u001b[1;32m    500\u001b[0m     data_type\u001b[39m=\u001b[39;49mdata_type,\n\u001b[1;32m    501\u001b[0m     async_job_plan\u001b[39m=\u001b[39;49mplan,\n\u001b[1;32m    502\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    503\u001b[0m )\n\u001b[1;32m    504\u001b[0m placeholders[query\u001b[39m.\u001b[39mquery_id_place_holder] \u001b[39m=\u001b[39m (\n\u001b[1;32m    505\u001b[0m     result[\u001b[39m\"\u001b[39m\u001b[39msfqid\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_last \u001b[39melse\u001b[39;00m result\u001b[39m.\u001b[39mquery_id\n\u001b[1;32m    506\u001b[0m )\n\u001b[1;32m    507\u001b[0m result_meta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cursor\u001b[39m.\u001b[39mdescription\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pysnowpark/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py:104\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[39mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[39m.\u001b[39mSERVER_SESSION_EXPIRED(\n\u001b[1;32m    101\u001b[0m         ex\u001b[39m.\u001b[39mcause\n\u001b[1;32m    102\u001b[0m     )\n\u001b[1;32m    103\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mraise\u001b[39;00m ex\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pysnowpark/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py:98\u001b[0m, in \u001b[0;36mServerConnection._Decorator.wrap_exception.<locals>.wrap\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[39mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[39m.\u001b[39mSERVER_SESSION_HAS_BEEN_CLOSED()\n\u001b[1;32m     97\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     99\u001b[0m \u001b[39mexcept\u001b[39;00m ReauthenticationRequest \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m    100\u001b[0m     \u001b[39mraise\u001b[39;00m SnowparkClientExceptionMessages\u001b[39m.\u001b[39mSERVER_SESSION_EXPIRED(\n\u001b[1;32m    101\u001b[0m         ex\u001b[39m.\u001b[39mcause\n\u001b[1;32m    102\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pysnowpark/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py:333\u001b[0m, in \u001b[0;36mServerConnection.run_query\u001b[0;34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, async_job_plan, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m     query_id_log \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m [queryID: \u001b[39m\u001b[39m{\u001b[39;00mex\u001b[39m.\u001b[39msfqid\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(ex, \u001b[39m\"\u001b[39m\u001b[39msfqid\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    332\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to execute query\u001b[39m\u001b[39m{\u001b[39;00mquery_id_log\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mquery\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mex\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 333\u001b[0m     \u001b[39mraise\u001b[39;00m ex\n\u001b[1;32m    335\u001b[0m \u001b[39m# fetch_pandas_all/batches() only works for SELECT statements\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[39m# We call fetchall() if fetch_pandas_all/batches() fails,\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[39m# because when the query plan has multiple queries, it will\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[39m# have non-select statements, and it shouldn't fail if the user\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[39m# calls to_pandas() to execute the query.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m block:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pysnowpark/lib/python3.8/site-packages/snowflake/snowpark/_internal/server_connection.py:317\u001b[0m, in \u001b[0;36mServerConnection.run_query\u001b[0;34m(self, query, to_pandas, to_iter, is_ddl_on_temp_object, block, data_type, async_job_plan, **kwargs)\u001b[0m\n\u001b[1;32m    315\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39m_statement_params\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mSNOWPARK_SKIP_TXN_COMMIT_IN_DDL\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[0;32m--> 317\u001b[0m     results_cursor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cursor\u001b[39m.\u001b[39;49mexecute(query, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    318\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnotify_query_listeners(\n\u001b[1;32m    319\u001b[0m         QueryRecord(results_cursor\u001b[39m.\u001b[39msfqid, results_cursor\u001b[39m.\u001b[39mquery)\n\u001b[1;32m    320\u001b[0m     )\n\u001b[1;32m    321\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExecute query [queryID: \u001b[39m\u001b[39m{\u001b[39;00mresults_cursor\u001b[39m.\u001b[39msfqid\u001b[39m}\u001b[39;00m\u001b[39m] \u001b[39m\u001b[39m{\u001b[39;00mquery\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pysnowpark/lib/python3.8/site-packages/snowflake/connector/cursor.py:804\u001b[0m, in \u001b[0;36mSnowflakeCursor.execute\u001b[0;34m(self, command, params, _bind_stage, timeout, _exec_async, _no_retry, _do_reset, _put_callback, _put_azure_callback, _put_callback_output_stream, _get_callback, _get_azure_callback, _get_callback_output_stream, _show_progress_bar, _statement_params, _is_internal, _describe_only, _no_results, _is_put_get, _raise_put_get_error, _force_put_overwrite, file_stream)\u001b[0m\n\u001b[1;32m    800\u001b[0m     is_integrity_error \u001b[39m=\u001b[39m (\n\u001b[1;32m    801\u001b[0m         code \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m100072\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    802\u001b[0m     )  \u001b[39m# NULL result in a non-nullable column\u001b[39;00m\n\u001b[1;32m    803\u001b[0m     error_class \u001b[39m=\u001b[39m IntegrityError \u001b[39mif\u001b[39;00m is_integrity_error \u001b[39melse\u001b[39;00m ProgrammingError\n\u001b[0;32m--> 804\u001b[0m     Error\u001b[39m.\u001b[39;49merrorhandler_wrapper(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconnection, \u001b[39mself\u001b[39;49m, error_class, errvalue)\n\u001b[1;32m    805\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pysnowpark/lib/python3.8/site-packages/snowflake/connector/errors.py:276\u001b[0m, in \u001b[0;36mError.errorhandler_wrapper\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    254\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merrorhandler_wrapper\u001b[39m(\n\u001b[1;32m    255\u001b[0m     connection: SnowflakeConnection \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     error_value: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mbool\u001b[39m \u001b[39m|\u001b[39m \u001b[39mint\u001b[39m],\n\u001b[1;32m    259\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m     \u001b[39m\"\"\"Error handler wrapper that calls the errorhandler method.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \n\u001b[1;32m    262\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[39m        exception to the first handler in that order.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m     handed_over \u001b[39m=\u001b[39m Error\u001b[39m.\u001b[39;49mhand_to_other_handler(\n\u001b[1;32m    277\u001b[0m         connection,\n\u001b[1;32m    278\u001b[0m         cursor,\n\u001b[1;32m    279\u001b[0m         error_class,\n\u001b[1;32m    280\u001b[0m         error_value,\n\u001b[1;32m    281\u001b[0m     )\n\u001b[1;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m handed_over:\n\u001b[1;32m    283\u001b[0m         \u001b[39mraise\u001b[39;00m Error\u001b[39m.\u001b[39merrorhandler_make_exception(\n\u001b[1;32m    284\u001b[0m             error_class,\n\u001b[1;32m    285\u001b[0m             error_value,\n\u001b[1;32m    286\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pysnowpark/lib/python3.8/site-packages/snowflake/connector/errors.py:331\u001b[0m, in \u001b[0;36mError.hand_to_other_handler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[39mif\u001b[39;00m cursor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     cursor\u001b[39m.\u001b[39mmessages\u001b[39m.\u001b[39mappend((error_class, error_value))\n\u001b[0;32m--> 331\u001b[0m     cursor\u001b[39m.\u001b[39;49merrorhandler(connection, cursor, error_class, error_value)\n\u001b[1;32m    332\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[39melif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pysnowpark/lib/python3.8/site-packages/snowflake/connector/errors.py:210\u001b[0m, in \u001b[0;36mError.default_errorhandler\u001b[0;34m(connection, cursor, error_class, error_value)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_errorhandler\u001b[39m(\n\u001b[1;32m    194\u001b[0m     connection: SnowflakeConnection,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m     error_value: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m],\n\u001b[1;32m    198\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     \u001b[39m\"\"\"Default error handler that raises an error.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39m        A Snowflake error.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(\n\u001b[1;32m    211\u001b[0m         msg\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmsg\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    212\u001b[0m         errno\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39merrno\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    213\u001b[0m         sqlstate\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39msqlstate\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    214\u001b[0m         sfqid\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39msfqid\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    215\u001b[0m         done_format_msg\u001b[39m=\u001b[39merror_value\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdone_format_msg\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m    216\u001b[0m         connection\u001b[39m=\u001b[39mconnection,\n\u001b[1;32m    217\u001b[0m         cursor\u001b[39m=\u001b[39mcursor,\n\u001b[1;32m    218\u001b[0m     )\n",
      "\u001b[0;31mSnowparkSQLException\u001b[0m: (1304): 01ab15d2-0402-b697-0057-3c03055c8bee: 100357 (P0000): Python Interpreter Error:\nTraceback (most recent call last):\n  File \"/home/udf/374669008373/delete_dag_for_datafile.py\", line 54, in main\n    tasks_affected = delete_taskdefinitions_for_datafile(p_session ,p_datafile ,p_drop_tasks)\n  File \"/home/udf/374669008373/delete_dag_for_datafile.py\", line 46, in delete_taskdefinitions_for_datafile\n    tasks_dropped.append(r['TASK_NAME'])\nUnboundLocalError: local variable 'r' referenced before assignment\n in function DELETE_DAG_FOR_DATAFILE with handler delete_dag_for_datafile.main"
     ]
    }
   ],
   "source": [
    "# Cautious enablement, used during development for testing\n",
    "print(f'Cleaning dags for datafile: {DATA_FILE_BASENAME_CLEANSED}')\n",
    "sp_session.call('delete_dag_for_datafile',DATA_FILE_BASENAME_CLEANSED ,False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89f1e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the warehouse size to desired\n",
    "\n",
    "print(f'''No of warehouses: {len(warehouses.split(','))}''')\n",
    "for wh in warehouses.split(','):\n",
    "    sp_session.sql(f''' alter warehouse {wh} set max_concurrency_level = 8; ''').collect()\n",
    "    sp_session.sql(f''' alter warehouse {wh} set warehouse_size = {warehouse_size}; ''').collect()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88e7dbf8",
   "metadata": {},
   "source": [
    "---\n",
    "## Data loading\n",
    "We will be loading the segments and file header using DAG. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ebc241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we build out the DAG\n",
    "df = sp_session.call('in_network_rates_dagbuilder_matrix' ,f'{INPUT_DATA_STAGE}/{DATA_STAGE_FOLDER}' ,DATA_FILE \n",
    "    ,f\"@{TARGET_DATA_STAGE}/{TARGET_FOLDER}\" ,SEGMENTS_PER_TASK ,warehouses ,5 ,5)\n",
    "\n",
    "sp_session.sql(f''' alter stage {TARGET_DATA_STAGE} refresh; ''').collect()\n",
    "print(' Status of execution')\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b494c8a",
   "metadata": {},
   "source": [
    "The above operation results in defining the DAG in Snowflake like here. The task names are specific to the data file being parsed.\n",
    "![](../../doc/soln_images/task_dags.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a701da57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we invoke the DAG\n",
    "\n",
    "start_time = time.time()\n",
    "print(f'Started at: {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "sql_stmts = [\n",
    "    f''' execute task DAG_ROOT_{DATA_FILE_BASENAME_CLEANSED}; '''\n",
    "]\n",
    "for stmt in sql_stmts:\n",
    "    print(stmt)\n",
    "    sp_session.sql(stmt).collect()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f'Ended at: {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "elapsed = str(timedelta(seconds=elapsed_time))\n",
    "print(f'Elapsed: {elapsed}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7465540",
   "metadata": {},
   "source": [
    "---\n",
    "## Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e8c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Tasks to segments')\n",
    "file_ingestion_df = sp_session.table('TASK_TO_SEGMENTIDS').filter(F.col('DATA_FILE') == F.lit(DATA_FILE)).to_pandas()\n",
    "display(file_ingestion_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8d3957",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Tasks ,warehouses and state')\n",
    "sp_session.sql(f''' SHOW TASKS IN  DATABASE {config['APP_DB']['database']}; ''').collect()\n",
    "stmt = f'''\n",
    "    select \"name\" as task_name\n",
    "        ,\"warehouse\" as warehouse\n",
    "        ,\"state\" as state\n",
    "    from table(result_scan(last_query_id()))\n",
    "    where \"name\" like '%{DATA_FILE_BASENAME_CLEANSED.upper()}%'\n",
    "       -- and state != 'suspended'\n",
    "    order by state;\n",
    "'''\n",
    "df = sp_session.sql(stmt).to_pandas()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6ffbf6",
   "metadata": {},
   "source": [
    "--- \n",
    "### Closeout\n",
    "\n",
    "    With that we are finished this section of the demo setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192cf880",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_session.close()\n",
    "print('Finished!!!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysnowpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "f66ab0cca93a35542579a5cebff8d4c56cda5b2ee20dfe6b405f4dd558333bf9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
