{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4022bce",
   "metadata": {},
   "source": [
    "# Load Demo with DAG\n",
    "\n",
    "In this we demonstrate loading of a sample data file [reduced_sample_data.json](../../data/reduced_sample_data.json) and showcase all the \n",
    "various resources that gets populated. We now are demonstrating using DAG\n",
    "\n",
    "#### Pre-requisite\n",
    "It is assumed that the setup steps are completed successfully. These are:\n",
    " - creating the database, schemas, stages\n",
    " - defining the external stage\n",
    " - defining the functions and stored procedures\n",
    " - defining the external tables and views\n",
    "\n",
    "If you had not done this, use the streamlit (./bin/run_app.sh) to create these as defined in the subpage 'Setup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f8f142c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Initialization"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CURRENT_USER()</th>\n",
       "      <th>CURRENT_ROLE()</th>\n",
       "      <th>CURRENT_DATABASE()</th>\n",
       "      <th>CURRENT_SCHEMA()</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VSEKAR</td>\n",
       "      <td>PUBLIC</td>\n",
       "      <td>INDSOL_CMSGOV_PRICING</td>\n",
       "      <td>PUBLIC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CURRENT_USER() CURRENT_ROLE()     CURRENT_DATABASE() CURRENT_SCHEMA()\n",
       "0         VSEKAR         PUBLIC  INDSOL_CMSGOV_PRICING           PUBLIC"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialization block\n",
    "from IPython.display import display, HTML, Image , Markdown\n",
    "from snowflake.snowpark.session import Session\n",
    "import snowflake.snowpark.types as T\n",
    "import snowflake.snowpark.functions as F\n",
    "import os ,configparser ,json ,logging\n",
    "\n",
    "# Import the commonly defined utility scripts using\n",
    "# dynamic path include\n",
    "import sys\n",
    "sys.path.append('../python/lutils')\n",
    "import sflk_base as L\n",
    "\n",
    "display(Markdown(\"### Initialization\"))\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.ERROR)\n",
    "\n",
    "# Source various helper functions\n",
    "%run ./scripts/notebook_helpers.py\n",
    "\n",
    "# Define the project home directory, this is used for locating the config.ini file\n",
    "PROJECT_HOME_DIR = '../../'\n",
    "config = L.get_config(PROJECT_HOME_DIR)\n",
    "sp_session = L.connect_to_snowflake(PROJECT_HOME_DIR)\n",
    "\n",
    "if(sp_session == None):\n",
    "    raise Exception(f'Unable to connect to snowflake. Validate connection information ')\n",
    "\n",
    "sp_session.use_role(f'''{config['APP_DB']['role']}''')\n",
    "sp_session.use_schema(f'''{config['APP_DB']['database']}.{config['APP_DB']['schema']}''')\n",
    "sp_session.use_warehouse(f'''{config['SNOW_CONN']['warehouse']}''')\n",
    "\n",
    "df = sp_session.sql('select current_user() ,current_role() ,current_database() ,current_schema();').to_pandas()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "90cd9156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def get_basename_of_datafile(p_datafile:str) -> str:\n",
    "    base = os.path.basename(p_datafile)\n",
    "    fl_base = os.path.splitext(base)\n",
    "    return fl_base[0]\n",
    "\n",
    "def get_cleansed_file_basename(p_datafile):\n",
    "    fl_basename = get_basename_of_datafile(p_datafile)\n",
    "    # Replace all non alphanumeric characters with _\n",
    "    fl_name = re.sub('[^0-9a-zA-Z]+', '_', fl_basename)\n",
    "    return fl_name\n",
    "\n",
    "# INPUT_DATA_STAGE = config['APP_DB']['ext_stage']\n",
    "INPUT_DATA_STAGE = 'data_stg'\n",
    "DATA_STAGE_FOLDER = config['APP_DB']['folder_data']\n",
    "\n",
    "DATA_FILE = '2022_10_01_priority_health_HMO_in-network-rates.zip'\n",
    "# DATA_FILE = 'reduced_sample_data.json'\n",
    "\n",
    "DATA_FILE_BASENAME = get_basename_of_datafile(DATA_FILE)\n",
    "DATA_FILE_BASENAME_CLEANSED = get_cleansed_file_basename(DATA_FILE)\n",
    "\n",
    "TARGET_DATA_STAGE = config['APP_DB']['ext_stage']\n",
    "TARGET_FOLDER = config['APP_DB']['folder_parsed']\n",
    "\n",
    "SEGMENTS_PER_TASK = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f908941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Cleanup block"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " truncating tables ...\n",
      " cleaning up files in external stage under path raw_parsed/2022_10_01_priority_health_HMO_in-network-rates/ ...\n"
     ]
    }
   ],
   "source": [
    "display(Markdown(\"### Cleanup block\"))\n",
    "# We will cleanup specific resources and artifacts from possible previous runs.\n",
    "\n",
    "stmts = [\n",
    "    f''' delete from segment_task_execution_status where data_file = '{DATA_FILE}'; '''\n",
    "    ,f''' delete from task_to_segmentids where data_file = '{DATA_FILE}'; '''\n",
    "    ,f''' delete from in_network_rates_file_header where data_file = '{DATA_FILE}'; '''\n",
    "    ,f''' delete from in_network_rates_segment_header where data_file = '{DATA_FILE}'; '''\n",
    "    ,f''' alter stage {INPUT_DATA_STAGE} refresh; '''\n",
    "]    \n",
    "    \n",
    "print(' truncating tables ...')\n",
    "for stmt in stmts:\n",
    "    sp_session.sql(stmt).collect()\n",
    "\n",
    "print(f''' cleaning up files in external stage under path {TARGET_FOLDER}/{DATA_FILE_BASENAME}/ ...''')\n",
    "\n",
    "stmt = f''' select relative_path from directory(@{TARGET_DATA_STAGE}) where relative_path like '%{DATA_STAGE_FOLDER}/{DATA_FILE_BASENAME}/%'; '''\n",
    "files = sp_session.sql(stmt).collect()\n",
    "for r in files:\n",
    "    stmt = f''' remove @{TARGET_DATA_STAGE}/{r['RELATIVE_PATH']}; '''\n",
    "    sp_session.sql(stmt).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88e7dbf8",
   "metadata": {},
   "source": [
    "---\n",
    "## Data loading\n",
    "We will be loading the segments and file header using DAG. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f2ebc241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Status of execution\n",
      "{\n",
      "  \"data_file\": \"2022_10_01_priority_health_HMO_in-network-rates.zip\",\n",
      "  \"root_task\": \"DAG_ROOT_2022_10_01_priority_health_HMO_in_network_rates\",\n",
      "  \"status\": true,\n",
      "  \"task_matrix_shape\": [\n",
      "    5,\n",
      "    15\n",
      "  ],\n",
      "  \"term_task\": \"TERM_tsk_2022_10_01_priority_health_HMO_in_network_rates\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# we build out the DAG\n",
    "df = sp_session.call('in_network_rates_dagbuilder' ,f'{INPUT_DATA_STAGE}/{DATA_STAGE_FOLDER}' ,DATA_FILE \n",
    "    ,f\"@{TARGET_DATA_STAGE}/{TARGET_FOLDER}\" ,SEGMENTS_PER_TASK ,config['SNOW_CONN']['warehouse'])\n",
    "\n",
    "sp_session.sql(f''' alter stage {TARGET_DATA_STAGE} refresh; ''').collect()\n",
    "print(' Status of execution')\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b494c8a",
   "metadata": {},
   "source": [
    "The above operation results in defining the DAG in Snowflake like here. The task names are specific to the data file being parsed.\n",
    "![](../../doc/soln_images/task_dags.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a701da57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 19:06:36\n",
      " alter warehouse DEMO_BUILD_WH set max_concurrency_level = 8 \n",
      " alter warehouse DEMO_BUILD_WH set warehouse_size = SMALL; \n",
      "Ended at: 19:06:38\n",
      "Elapsed: 0:00:01.469378\n"
     ]
    }
   ],
   "source": [
    "# Next we invoke the DAG\n",
    "\n",
    "start_time = time.time()\n",
    "print(f'Started at: {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "sql_stmts = [\n",
    "    # f''' alter warehouse {config['SNOW_CONN']['warehouse']} set max_concurrency_level = 8 '''\n",
    "    # XSMALL | SMALL | MEDIUM | LARGE | XLARGE | XXLARGE | XXXLARGE | X4LARGE | X5LARGE | X6LARGE\n",
    "    ,f''' alter warehouse {config['SNOW_CONN']['warehouse']} set warehouse_size = SMALL; '''\n",
    "    ,f''' execute task DAG_ROOT_{DATA_FILE_BASENAME_CLEANSED}; '''\n",
    "]\n",
    "for stmt in sql_stmts:\n",
    "    print(stmt)\n",
    "    sp_session.sql(stmt).collect()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f'Ended at: {datetime.now().strftime(\"%H:%M:%S\")}')\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "elapsed = str(timedelta(seconds=elapsed_time))\n",
    "print(f'Elapsed: {elapsed}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7465540",
   "metadata": {},
   "source": [
    "---\n",
    "## Inspection (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d88012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "continue_sleeping = True\n",
    "\n",
    "while continue_sleeping == True: \n",
    "    time.sleep(2*60)\n",
    "\n",
    "    sql_stmt = f'''\n",
    "        select *\n",
    "        from current_segment_parsing_tasks_v\n",
    "        where l.data_file = '{DATA_FILE}'\n",
    "        ;\n",
    "    '''\n",
    "    df_running = sp_session.table('current_segment_parsing_tasks_v').to_pandas()\n",
    "    \n",
    "    len_running_count = len(df_running)\n",
    "    print(f'Current running tasks [{len_running_count}]...')\n",
    "    display(df_running)\n",
    "    \n",
    "    sql_stmt = f'''\n",
    "        select \n",
    "            count(l.*) as l_count\n",
    "            ,count(r.*) as r_count\n",
    "            ,l_count - r_count as row_count_diff\n",
    "            -- l.*\n",
    "        from segment_task_execution_status as l\n",
    "            full outer join segments_counts_for_datafile_v as r\n",
    "                on r.task_name = l.task_name\n",
    "        where not (l.task_name  like any ('DAG_%' ,'TERM_%' ,'%T_FH_%' ))\n",
    "            and JSON_EXTRACT_PATH_TEXT(l.task_ret_status ,'task_ignored_parsing') is null\n",
    "            and l.data_file = '{DATA_FILE}'\n",
    "    '''\n",
    "    df = sp_session.sql(sql_stmt).to_pandas()\n",
    "    row_count = df['ROW_COUNT_DIFF'][0]\n",
    "    finished_row_count = df['R_COUNT'][0]\n",
    "    print(f'Finished tasks [{row_count}] ...')\n",
    "    if row_count == 0:\n",
    "        print('Finished sleeping ...')\n",
    "        continue_sleeping = False\n",
    "        break\n",
    "\n",
    "    display(df)\n",
    "    print('\\n ------------------------------ ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8fa5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"The table in_network_rates_file_header holds the file header elements\"))\n",
    "\n",
    "df = (sp_session.table('in_network_rates_file_header')\n",
    "        .filter(F.col('DATA_FILE') == F.lit(DATA_FILE))\n",
    "        .to_pandas())\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664bef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' The HEADER has the following data ')\n",
    "json.loads(df['HEADER'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d38b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"The table in_network_rates_segment_header lists the various segments that were loaded\"))\n",
    "\n",
    "df = (sp_session.table('in_network_rates_segment_header')\n",
    "        .filter(F.col('DATA_FILE') == F.lit(DATA_FILE))\n",
    "        .sample(n=5)\n",
    "        .to_pandas())\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdcc96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' The NEGOTIATED_RATES_INFO has the following data ')\n",
    "json.loads(df['NEGOTIATED_RATES_INFO'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ae7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "para = f''' \n",
    "The parsed data are stored as parquet files in the external stage at: @{TARGET_DATA_STAGE}/{TARGET_FOLDER}. The directory structure follows the format:\n",
    "\n",
    "<ext stage>/<folder_parsed>/<data file basename (ex: reduced_sample_data)/<segment_id>/<segment type (negotiated_rates)>/data_<seq_no>_<chunk_no>.parquet.gz\n",
    "\n",
    "Description:\n",
    "- ext stage : external stage name\n",
    "- folder_parsed : configured value of APP_DB.folder_parsed in config.ini\n",
    "- data file basename : the data file basename, without the file extension\n",
    "- segment_id : a unique identifier for the segment, this is a composite key of <negotiation_arrangement>::<billing_code_type>::<billing_code>::<billing_code_type_version>\n",
    "- segment type : indicates the segment children type, this could be either of negotiated_rates/bundled_codes/covered_services\n",
    "- The file which will contain the record, stored in parquet file.\n",
    "'''\n",
    "print(para)\n",
    "\n",
    "stmt = f''' select relative_path from directory(@{TARGET_DATA_STAGE}) where relative_path like '%{TARGET_FOLDER}/{DATA_FILE_BASENAME}%' limit 5; '''\n",
    "df = sp_session.sql(stmt).to_pandas()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f014bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "para = f''' \n",
    "The parquet file can be queried inside Snowflake via external table: ext_negotiated_arrangments_staged. Each of the folder structure is partitioned to seperate columns , which can help with pruning to specific segments.\n",
    "The value contains the negotiated_rates and other children elements stored in JSON format\n",
    "'''\n",
    "print(para)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 25)\n",
    "df = (sp_session.table('ext_negotiated_arrangments_staged')\n",
    "        .filter(F.col('P_DATA_FL') == F.lit(DATA_FILE_BASENAME))\n",
    "        .sample(n=5)\n",
    "        .to_pandas())\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522943b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' a sample view of one of the records')\n",
    "j = json.loads(df['VALUE'][0])\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8769a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"The table segment_task_execution_status contains the audit of execution for the above stored procedures\"))\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df = (sp_session.table('segment_task_execution_status')\n",
    "        .filter(F.col('DATA_FILE') == F.lit(DATA_FILE))\n",
    "        .sample(n=5)\n",
    "        .to_pandas())\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9bf2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"The view segments_counts_for_datafile_v, based of table segment_task_execution_status, will help to identify the number of negotiated_arrangement segments for a specific data file. This view is populated once all the segments in a specific data files are parsed out\"))\n",
    "\n",
    "df = (sp_session.table('segments_counts_for_datafile_v')\n",
    "        .filter(F.col('DATA_FILE') == F.lit(DATA_FILE))\n",
    "        .sample(n=5)\n",
    "        .to_pandas())\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6ffbf6",
   "metadata": {},
   "source": [
    "--- \n",
    "### Closeout\n",
    "\n",
    "    With that we are finished this section of the demo setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192cf880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp_session.close()\n",
    "print('Finished!!!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysnowpark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "f66ab0cca93a35542579a5cebff8d4c56cda5b2ee20dfe6b405f4dd558333bf9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
